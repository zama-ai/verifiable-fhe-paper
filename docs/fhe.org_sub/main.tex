% \documentclass{article}
\documentclass[9pt]{extarticle}

\usepackage{mathtools}
\usepackage{mathcalbd} % for \pol macro
\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage[margin=3.4cm]{geometry}
% \usepackage{fullpage}
% \usepackage[tracking=true]{microtype}
% \DeclareMicrotypeSet*[tracking]{my}{font = */*/*/sc/*}
% \SetTracking{encoding = *, shape = sc}{45}


\usepackage[colorlinks]{hyperref} % should be loaded last
\usepackage{cleveref}  % should be loaded last last

\title{Towards Verifiable Bootstrapping in Practice \\ \large{Proving Correct Execution of TFHE's Blind Rotation using plonky2}}

\author{Michael Walter\\
  \texttt{michael.walter@zama.ai}\\
  Zama, France
  \and
  Louis Tremblay Thibault\\
  \texttt{louis.tremblay.thibault@zama.ai}\\
  Zama, France
}

\date{}

% --- Macros
\newcommand{\ints}{\mathds{Z}}
% \newcommand{\reals}{\mathds{R}}
% \newcommand{\torus}{\mathds{T}}
% \newcommand*{\pol}{\mathcalbd}
% \newcommand*{\SQ}{\mathit{S\!Q}}
\newcommand{\field}{\mathbb{F}}

\begin{document}

\maketitle


\section{Introduction}\label{sec:introduction}
There are two emerging cryptographic technologies with a host of applications in practice: Fully Homomorphic Encryption (FHE) and Succinct Non-interactive Arguments of Knowledge (SNARKs). FHE allows arbitrary computation on encrypted data, while SNARKs enable proving the correct execution of arbitrary computation with short proofs and sublinear verification time. It is not hard to see the vast number of possible use cases for each of these technologies in practice, but in this work we are interested in the combination of the two. Specifically, we investigate to what extent it is possible to prove the correct execution of FHE operations using SNARKs in practice.

Combining FHE with SNARKs is enormously appealing as this has the potential to entirely replace solutions to secure outsourcing of computing based on hardware modules, which are riddled with practical attacks and ultimately only achieve a shift of trust to the hardware vendor. In contrast, a verified FHE scheme would allow outsourcing computation and reduce trust to cryptographic, i.e., mathematical, assumptions using minimal interaction. Furthermore, such a scheme would thwart CCA-style attacks, to which FHE schemes are known to be inherently vulnerable \cite{EPRINT:ChiGamGou16}, which means practical deployment needs to be very prudent in its use of FHE in order not to fall victim to attacks outside of the security model.

Unfortunately, despite a large amount of research and significant progress over the past one and a half decades, FHE operations still incur a significant overhead over their cleartext counterparts. Even worse, any truly \emph{fully} homomorphic scheme we know of to date relies on a bootstrapping operation to reduce noise in ciphertexts, which accumulates during homomorphic operations and may lead to incorrect decryption if not handled correctly. In all current FHE schemes, this bootstrapping is the costliest operation.

On the other hand, SNARKs themselves incur a significant overhead over the computation to prove, many practical SNARKs having proving complexity superlinear in the size of the computation.\footnote{With \emph{size} we mean here the size of the circuit used to perform the computation in the arithmetic circuit model.} Furthermore, since the proof generation typically requires to keep the entire trace in memory,\footnote{There are techniques to mitigate this issue for \emph{structured} computations. We will come back to this later.} the memory requirement of SNARKs grows at least linearly in the computation length, which renders the memory complexity the bottleneck for long computations.

So it is not surprising that the bootstrapping operation represents a formidable challenge for SNARKs. While some works have considered proving \emph{levelled} homomorphic operations \cite{viand2023verifiable,EPRINT:GanNitSor21}, as far as we are aware, there are no published attempts of applying a SNARK to an FHE bootstrapping operation, let alone successful attempts. In this work we seek to remedy this state of affairs and demonstrate progress towards a fully verifiable bootstrapping. Our results suggest that generating a proof for a bootstrapping operation is within reach in practice.

\section{Choice of FHE scheme and SNARK}
\label{sec:fhe_snark}

\subsection{FHE Scheme}
\label{sec:fhe}


Since our goal is fairly ambitious, we try to make our lives as easy as possible. In particular, we choose as our target the FHE scheme with the lightest known bootstrapping operation, namely TFHE. At its core, this bootstrapping consists of a \emph{blind rotation}, which is a loop of length $n$ of homomorphic CMUX style operations on polynomials in $R_q = \ints_q[X] / (X^N + 1)$ for some modulus $q$ and ring dimension~$N$. Each of these CMUX operations consists of a multiplication of the input polynomial by a monomial of the form $X^a$ (where $a \in \ints_{2N}$ is part of the input), approximate radix decompositions of polynomials with base $B$, and $k + 1$ polynomial inner products of size $\ell$ with the ($i$-th element of the) bootstrapping key. For this extended abstract we will not go further into the details of TFHE's bootstrapping and refer to \cite{JC:CGGI20,AC:CLOT21,EPRINT:BBBCLOT22} instead. TFHE is typically instantiated with the ciphertext modulus $q=2^{64}$ (see e.g.\ \cite{AC:CGGI16,TFHE-rs}). The other parameters are typically the result of a complex optimization procedure, but for concreteness the reader may think of $n = 2^9$, $N=2^{10}$, $k=1$, $B = 2^8$ and $\ell=2$, which gives an idea of realistic parameters. We will use this set of parameters as a running example in this work.

Finally, we take the liberty to modify TFHE at a few places to make it more amendable to our target SNARK. These modifications maintain the functionality of the bootstrap, but might make it slightly less efficient. If the modifications yield a faster proof generation, this is likely a worthwhile trade-off depending on the overall system. The most significant modification we apply is to use a SNARK-friendly prime modulus $q \approx 2^{64}$ instead of a power of 2, because most efficient SNARKs only natively support arithmetic circuits over fields.\footnote{There are attempts to construct SNARKs for ring arithmetic \cite{EPRINT:GanNitSor21}, but this comes with its own caveats, like designated verifier and relatively poor performance.} This way we avoid emulating the arithmetic in the ring $\ints_{2^{64}}$ within the SNARK field. 

\subsection{SNARK}
\label{sec:snark}
There are a number of SNARK implementations available and we selected the SNARK for our work based on the following criteria. In order to enable as many applications as possible, we target a transparent, publicly verifiable SNARK with sublinear verifier. For efficiency reasons, we require native support for arithmetic in fields of size $\approx 2^{64}$ and, ideally, support for efficiency improvements for structured computation like loops, since the blind rotation is essentially a large loop.

With these criteria in mind, plonky2 \cite{plonky2} provides a suitable candidate. It relies on the PLONK arithmetization \cite{EPRINT:GabWilCio19} in combination with a polynomial commitment scheme based on hash functions, namely FRI \cite{ICALP:BBHR18}. It uses as a base field $\field_p$ with $p = 2^{64} - 2^{32} + 1$, which meets our requirement on the modulus, and, as an added bonus, is plausibly post-quantum secure, which is also true for TFHE. plonky2 is optimized for recursion, which allows us to construct \emph{incrementally verifiable computation} (IVC) \cite{TCC:Valiant08}, a technique to prove loops more efficiently than simply rolling them out in a circuit, which will come in handy.

\section{Blind Rotation in plonky2}
\label{sec:blind}
To set a baseline, we implemented a basic version of one step of the blind rotation with above parameters\footnote{The one exception is that here we used the modulus $q=2^{64}$.} using RISC Zero's general purpose Virtual Machine\footnote{\url{https://www.risczero.com/}} and ran their compute cluster Bonsai on it. It was able to prove one step in about 5 minutes. Given the complexity of the circuit and that RISC Zero's VM is general purpose, this result speaks to its power to prove relatively long computations. It is hard to extrapolate this datapoint to a full blind rotation, since the VM may not run in linear time in the length of the computation and memory consumption is likely to be an issue at some point. In the following, we explore what can be achieved by crafting a circuit specifically for a step of the blind rotation and how to scale it to a full blind rotation.

\subsection{One Step of the Blind Rotation}
\label{sec:step}

One of the bottleneck operations during a step of the blind rotation is polynomial multiplication in $R_q$. Implementations like \cite{TFHE-rs} or the one accompanying \cite{AC:CGGI16} use an FFT on floating point numbers, which are very inefficient to realize in the arithmetic circuit model. Luckily, the choice of modulus $p = 2^{64} - 2^{32} + 1$ admits performing this multiplication using the NTT, so here we diverge from common implementations and use an NTT circuit instead.

The second main operation is multiplication by the monomial $X^a$, where $a \in \ints_{2N}$ is an input. This corresponds to a negacyclic rotation by $a$ in the ring $R_q$, which is a rather trivial (and linear) operation on a CPU. However, in the circuit model it is not quite as easy, since $a$ is not known during circuit construction and we cannot ``rewire'' a circuit during evaluation. Note that this operation would be trivial in the circuit model, if $a$ was a fixed constant. So our solution to this problem is to implement subcircuits for negacyclic rotations by powers of two. Then we apply each of the subcircuits and each time select the rotated or not rotated polynomial using a CMUX and the corresponding bit of the binary decomposition of $a$ as control bit. This is a circuit of size $O(N \log N)$ and thus significantly more expensive than on a CPU. All other operations (addition, decomposition) are readily available in plonky2 and are easily generalized to polynomials.

\paragraph{Preliminary Results}
Using a standard laptop we were able to prove one step of the blind rotation (with the parameters laid out in \Cref{sec:fhe} and some minor modifications) in about $15$s. Proof size was about $250$kb (although this can be reduced using recursion) and verification took about $15$ms. 

\subsection{Scaling to Full Blind Rotation}
\label{sec:full}
The obvious way to scaling the blind rotation step to $n$ steps is to build a large circuit with $n$ subcircuits performing one step each. While this works in theory, the circuit size blows up, since $n$ is very large. In fact, in our experiments we were only able to do this up to $n = 4$ (proving time about $50$s, proof size $460$kb, verification time $34$ms). For larger $n$ the laptop (with $8$GB of RAM) ran out of memory, which seems to become the bottleneck.

Another easy approach to scaling the blind rotation is to simply prove each step individually and send the proofs and intermediate results to the verifier. The verifier can check each of the proofs. This achieves a proving time that is linear in the number of steps and can be performed with memory equivalent to one step, so our test laptop could prove a blind rotation with $n=2^9$ in about $2$ hours. The issue with this approach is the proof size and verifier complexity: the proof now consists of $n$ smaller proofs and $n$ GLWE ciphertexts (the intermediate results) and the verifier needs to check all individual proofs. With our example parameters, each ciphertext has size about $4$ kb, so $n=2^9$ ciphertexts alone amount to about $2$ mb, without even considering the substantially larger proofs.\footnote{We remark that it might be possible to compress the set of proofs into a single smaller proof using recursion, but this will certainly not work for the intermediate results, which need to be sent and checked in any case.} In some applications this might be acceptable, but typically this is considered too large and the burden on the verifier too costly.

Clearly, we can use a hybrid strategy to reduce the proof size and verifier complexity. If we are working on a machine that is able to prove $t$ steps of the blind rotation at a time, we may take advantage of this and cut the number of intermediate results and inner proofs down by a factor $t$. For example, our test laptop was able to prove $t=4$ steps at a time, which means we could reduce the number of proofs and intermediate results from $n=2^9$ to $n=2^7$. A bigger machine would presumably be able to reduce this number even further.

\subsubsection{Blind Rotation Based on IVC}
\label{sec:ivc}
As noted in \Cref{sec:snark}, plonky2 supports recursion and thus allows constructing IVC. The general idea of IVC to prove a loop is the following. Let $F$ be the function describing the step function of the loop, i.e., we want to proof $y = F^{n}(x)$, where $F^{n}(x)$ corresponds to applying $F$ successively $n$ times to $x$. We may augment $F$ to obtain a function $F'$ that takes as input a value $y_i$ and a proof $\pi_i$ and outputs $y_{i+1}$ and a proof $\pi_{i+1}$ attesting to the correctness of the combined statement: 1) $y_{i+1} = F(y_i)$ and 2) the verifier accepts $\pi_i$ as a proof of $y_{i} = F(x)$. The prover may now successively evaluate $(y_i, \pi_i) = F'(y_{i-1}, \pi_{i-1})$ and obtain the output $y_{n}$ along with a succinct proof $\pi_{n}$. See e.g. \cite{DBLP:journals/ftsec/Thaler22} for more details and references.

This approach seems like the ideal tool to prove a blind rotation. The overhead for the prover of proving the verifier circuit for each iteration is relatively small compared to our step function, due to plonky2's focus on optimization of recursion. Unfortunately, there is another more subtle overhead. Note that in our description above there is no public input beyond the initial $x$. In particular, the individual loop iterations do not receive any public input specific to the iteration. In contrast, in our application of blind rotation, every loop iteration receives a different part of the bootstrapping key (and LWE mask element). The way we solve this is by passing the entire bootstrapping key as input to the step function and use a counter that keeps track of the loop iteration. Then we use a selector subcircuit that picks out the correct part of the key and the LWE mask for the current iteration. Note that this subcircuit grows linearly with the size of the bootstrapping key, which consists of $n (k+1)^2 \ell N$ elements in $\ints_q$. For small $n$ this circuit is smaller than the circuit for our step function, but as $n$ grows it quickly becomes the bottelneck. Still, since this circuit is concretely smaller than the one for $n$ plain subsequent bootstrapping steps, we can still use this to increase the number of steps provable with some given resources.

\paragraph{Preliminary Results}
Implementing an IVC based blind rotation and using the same test laptop as in \Cref{sec:step}, we were able to prove a blind rotation with $n=32$, where each step required about a minute to prove. The increased proving time per step compared to the $15$s from \Cref{sec:step} is mostly due to the overhead of the selection subcircuit. The proof size increased to $2$mb, but can be compressed to about $160$kb using recursion in about half a minute. The compressed proof took about $10$ms to verify. We note that it is possible to reduce the proof size and verification time even further at the cost of increased proving time by optimizing plonky2's parameters for proof size in the final recursive compression step and/or adding more recursive layers. In fact, the white paper associated to \cite{plonky2} claims a proof size of about $43$kb. We leave this optimization for future work. Applying the same hybrid strategy as in \Cref{sec:step}, the prover now only sends $16$ intermediate results and proofs to achieve $n=2^9$, which constitutes an improvement of a factor $8$. This comes at the cost of increased proving time, which now takes about $8$ hours. Clearly, other trade-offs are possible.

\subsection{Conclusion and Outlook}
\label{sec:outlook}

\paragraph{Remaining Work}
In the upcoming months, we plan to improve our circuit design, extending it to the full PBS including the modulus switch, key switch and sample extraction. We will experiment with different parameter sets and perform more conclusive benchmarks using a more suitable machine than a standard laptop. We will make our code publicly available and results from this further work will be featured in the talk proposed by this extended abstract.

\paragraph{Conclusions}
Even though our experimental results are preliminary at this point, we may already draw some conclusions. Our work demonstrates for the first time that a fully verifiable bootstrapping in practice is within reach. While performance is still likely to be too costly for many applications, others might already be able to take advantage of a fully verified FHE scheme. As an example, consider a blockchain protocol that allows smart contracts on encrypted data \cite{fhevm,phenix}. Here, verifiable FHE operations have the potential to replace certain consensus protocols and thus reduce the overall amount of computation that needs to be performed. A proving time of a few hours is acceptable in this setting, if this is deployed akin to optimistic roll-ups, where a proof is only required in case of a dispute.

\paragraph{Bottlenecks and Future Work}
As outlined above, the fact that the step function in our IVC based blind rotation needs to receive the entire bootstrapping key and select the current part using a circuit limits its performance significantly. Accordingly, any technique that reduces the size of the bootstrapping key is likely to improve the performance of the SNARK. Alternatively, the prover for blind rotation will benefit from SNARK techniques that allow proving repetitive computation with different public inputs for each iteration efficiently.

We also note that there is another promising approach to proving structured computation based on folding \cite{EPRINT:BowGriHop19,C:KotSetTzi22,EPRINT:KotSet22,cryptoeprint:2023/620,C:BCLMS21,TCC:BCMS20,sangria}. The crucial limitation of folding is that it requires a homomorphic polynomial commitment scheme (PCS). Unfortunately, any homomorphic PCS we are aware of that is suitable for folding uses large fields of size at least $ 2^{256}$, which would require to either emulate the ring $\ints_{2^{64}}$ or increase the ciphertext modulus $q$ of TFHE, depriving it of its advantage of working only with word-sized numbers on common architectures. Furthermore, these PCSs result in SNARKs with trusted setup or inefficient verifier and are not post-quantum secure. As far as we know, constructing a homomorphic, transparent PCS with efficient verifier and natively supporting small fields is still an open problem.

% --- Bibliography
\bibliographystyle{alphaurl}
%\bibliographystyle{abbrv}
\bibliography{../../../cryptobib/abbrev3,../../../cryptobib/crypto,local}

\end{document}
